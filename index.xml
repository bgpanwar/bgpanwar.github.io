<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Pond</title>
    <link>/</link>
    <description>Recent content on Data Pond</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 03 Jul 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>p-value Analysis</title>
      <link>/post/p-value-analysis/</link>
      <pubDate>Mon, 03 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/p-value-analysis/</guid>
      <description>if(!require(devtools)) install.packages(&amp;quot;devtools&amp;quot;)## Loading required package: devtools## Warning: package &amp;#39;devtools&amp;#39; was built under R version 3.3.3devtools::install_github(&amp;quot;kassambara/ggpubr&amp;quot;)## Skipping install of &amp;#39;ggpubr&amp;#39; from a github remote, the SHA1 (cb0f308d) has not changed since last install.## Use `force = TRUE` to force installation#install.packages(&amp;quot;ggpubr&amp;quot;)#install.packages(&amp;quot;dplyr&amp;quot;)if (!require(&amp;quot;ggpubr&amp;quot;)) {install.packages(&amp;quot;ggpubr&amp;quot;, repos=&amp;quot;http://cran.rstudio.com/&amp;quot;) library(&amp;quot;ggpubr&amp;quot;)}## Loading required package: ggpubr## Loading required package: ggplot2## Warning: package &amp;#39;ggplot2&amp;#39; was built under R version 3.</description>
    </item>
    
    <item>
      <title>Association Rules</title>
      <link>/post/association-rules-rmd/</link>
      <pubDate>Sun, 02 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/association-rules-rmd/</guid>
      <description>Association Rules-identifying frequently purchased groceries with association rules. market basket analysis will utilize the purchase data collected from one month of operation at a real-world grocery store. The data contains 9,835 transactions or about 327 transactions per day (roughly 30 transactions per hour in a 12-hour business day), suggesting that the retailer is not particularly large, nor is it particularly small.
The typical grocery store offers a huge variety of items.</description>
    </item>
    
    <item>
      <title>Clustering Analysis</title>
      <link>/post/clustering-analysis/</link>
      <pubDate>Sun, 02 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/clustering-analysis/</guid>
      <description>The cluster Analysis is performed on the SNS dataset.Clustering with k-means can be applied by identifying groups. Clustering can automate the process of discovering the natural segments in this population. However, it will be up to us to decide whether or not the clusters are interesting and how we can use them for advertising.
SNS dataset representing a random sample of 30,000 U.S. high school students who had profiles on a well-known SNS in 2006.</description>
    </item>
    
    <item>
      <title>Logistic Regression</title>
      <link>/post/logistic-regression/</link>
      <pubDate>Sun, 02 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/logistic-regression/</guid>
      <description>Following is the Logistic Regression analysis of the Challenger data.
## Example: Space Shuttle Launch Data ----launch &amp;lt;- read.csv(&amp;quot;C:/Users/Bhawna/Documents/blog/data/challenger.csv&amp;quot;)# examine the launch datastr(launch)## &amp;#39;data.frame&amp;#39;: 23 obs. of 4 variables:## $ distress_ct : int 0 1 0 0 0 0 0 0 1 1 ...## $ temperature : int 66 70 69 68 67 72 73 70 57 63 ...## $ field_check_pressure: int 50 50 50 50 50 50 100 100 200 200 .</description>
    </item>
    
    <item>
      <title>Naive Bayes</title>
      <link>/post/naive-bayes/</link>
      <pubDate>Sun, 02 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/naive-bayes/</guid>
      <description>The HouseVotes84 dataset was used for applying the naive Bayes algorithm.
library (e1071)## Naive Bayes Classifier for Discrete Predictors: we use again the Congressional Voting Records of 1984# Note refusals to vote have been treated as missing values!data (HouseVotes84, package=&amp;quot;mlbench&amp;quot;) model &amp;lt;- naiveBayes(Class ~ ., data = HouseVotes84) head(HouseVotes84)## Class V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15## 1 republican n y n y y y n n n y &amp;lt;NA&amp;gt; y y y n## 2 republican n y n y y y n n n n n y y y n## 3 democrat &amp;lt;NA&amp;gt; y y &amp;lt;NA&amp;gt; y y n n n n y n y y n## 4 democrat n y y n &amp;lt;NA&amp;gt; y n n n n y n y n n## 5 democrat y y y n y y n n n n y &amp;lt;NA&amp;gt; y y y## 6 democrat n y y n y y n n n n n n y y y## V16## 1 y## 2 &amp;lt;NA&amp;gt;## 3 n## 4 y## 5 y## 6 y# predict the outcome of the first 20 recordspredict(model, HouseVotes84[1:20,-1]) ## [1] republican republican republican democrat democrat democrat ## [7] republican republican republican democrat republican republican## [13] democrat democrat republican republican democrat democrat ## [19] republican democrat ## Levels: democrat republican# same but displaying posteriorspredict(model, HouseVotes84[1:20,-1], type = &amp;quot;raw&amp;quot;) ## democrat republican## [1,] 1.</description>
    </item>
    
    <item>
      <title>Principle Component Analysis</title>
      <link>/post/principle-component-analysis/</link>
      <pubDate>Sun, 02 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/principle-component-analysis/</guid>
      <description>Principle Component Analysis was performed using the Iris dataset.
# Load datadata(iris)head(iris, 3)## Sepal.Length Sepal.Width Petal.Length Petal.Width Species## 1 5.1 3.5 1.4 0.2 setosa## 2 4.9 3.0 1.4 0.2 setosa## 3 4.7 3.2 1.3 0.2 setosa# log transform log.ir &amp;lt;- log(iris[, 1:4])ir.species &amp;lt;- iris[, 5]# apply PCA - scale. = TRUE is highly # advisable, but default is FALSE. ir.pca &amp;lt;- prcomp(log.</description>
    </item>
    
    <item>
      <title>Random Forest Analysis</title>
      <link>/post/random-forest-analysis/</link>
      <pubDate>Sun, 02 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/random-forest-analysis/</guid>
      <description>The Random Forest analysis techniques was used to improve prediction on the German Credit Dataset.
The dataset is found:
url=&amp;quot;http://freakonometrics.free.fr/german_credit.csv&amp;quot;credit=read.csv(url, header = TRUE, sep = &amp;quot;,&amp;quot;)str(credit)## &amp;#39;data.frame&amp;#39;: 1000 obs. of 21 variables:## $ Creditability : int 1 1 1 1 1 1 1 1 1 1 ...## $ Account.Balance : int 1 1 2 1 1 1 1 1 4 2 ...## $ Duration.of.Credit..month. : int 18 9 12 12 12 10 8 6 18 24 .</description>
    </item>
    
    <item>
      <title>Statistical Analysis-Examples</title>
      <link>/post/statistical-analysis-examples/</link>
      <pubDate>Sun, 02 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/statistical-analysis-examples/</guid>
      <description>Statistical Analysis consists of code from the Chapter Lab Sections 4.6.1 and 4.6.2(Induction to Statitical book)
library (ISLR)## Warning: package &amp;#39;ISLR&amp;#39; was built under R version 3.3.3names(Smarket )## [1] &amp;quot;Year&amp;quot; &amp;quot;Lag1&amp;quot; &amp;quot;Lag2&amp;quot; &amp;quot;Lag3&amp;quot; &amp;quot;Lag4&amp;quot; &amp;quot;Lag5&amp;quot; ## [7] &amp;quot;Volume&amp;quot; &amp;quot;Today&amp;quot; &amp;quot;Direction&amp;quot;dim(Smarket )## [1] 1250 9summary (Smarket )## Year Lag1 Lag2 ## Min. :2001 Min. :-4.922000 Min. :-4.922000 ## 1st Qu.:2002 1st Qu.:-0.639500 1st Qu.:-0.639500 ## Median :2003 Median : 0.</description>
    </item>
    
    <item>
      <title>logistic Regression2</title>
      <link>/post/logistic-regression2/</link>
      <pubDate>Sun, 02 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/logistic-regression2/</guid>
      <description>Logistic Regression was used to run the Credit dataset.
credit &amp;lt;- read.csv(&amp;quot;C:/Users/Bhawna/Documents/blog/data/credit.csv&amp;quot;)# examine the launch datastr(credit)## &amp;#39;data.frame&amp;#39;: 1000 obs. of 17 variables:## $ checking_balance : Factor w/ 4 levels &amp;quot;&amp;lt; 0 DM&amp;quot;,&amp;quot;&amp;gt; 200 DM&amp;quot;,..: 1 3 4 1 1 4 4 3 4 3 ...## $ months_loan_duration: int 6 48 12 42 24 36 24 36 12 30 ...## $ credit_history : Factor w/ 5 levels &amp;quot;critical&amp;quot;,&amp;quot;good&amp;quot;,.</description>
    </item>
    
    <item>
      <title>About Data Pond</title>
      <link>/about/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>My past experience composed of system design and developing complex systems. In the process of reproducing failure and finding root cause, I fell in love with analyzing the data. That was the start of my love for applied statistics.
Thanks for reading!</description>
    </item>
    
    <item>
      <title>Artificial Neural Networks</title>
      <link>/post/ann-rmd/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/ann-rmd/</guid>
      <description>Artificial Neural Networks(ANN) Algorithm is used on Concrete dataset. Neural Networks are considered a black box process. ANNs are based on complex mathematical systems. But not a zero node NN is an alternative representation of the simple linear regression model.
ANNs are versatile learners that can be applied to nearly any learning task: classification, numeric prediction, and even unsupervised pattern recognition.
ANNs are best applied to problems where the input data and the output data are well-understood or at least fairly simple, yet the process that relates the input to the output is extremely complex.</description>
    </item>
    
    <item>
      <title>Scatter</title>
      <link>/post/scatter/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/scatter/</guid>
      <description>Load the data.
subscribers &amp;lt;- read.csv(&amp;quot;C:/Users/Bhawna/Documents/blog/data/flowingdata_subscribers.csv&amp;quot;, sep=&amp;quot;,&amp;quot;, header=TRUE)View(subscribers)Default plot with points.
plot(subscribers$Subscribers)Default plot with type explicity specified.
plot(subscribers$Subscribers, type=&amp;quot;p&amp;quot;, ylim=c(0, 30000))Draw vertical lines.
plot(subscribers$Subscribers, type=&amp;quot;h&amp;quot;, ylim=c(0, 30000))Draw points with above lines.
plot(subscribers$Subscribers, type=&amp;quot;h&amp;quot;, ylim=c(0, 30000))points(subscribers$Subscribers)Edits with colors and labels.
plot(subscribers$Subscribers, type=&amp;quot;h&amp;quot;, ylim=c(0, 30000), xlab=&amp;quot;Day&amp;quot;, ylab=&amp;quot;Subscribers&amp;quot;)points(subscribers$Subscribers, pch=19, col=&amp;quot;blue&amp;quot;) Reference:The data was downloaded from the following website website.</description>
    </item>
    
    <item>
      <title>Spam Filter</title>
      <link>/post/spam-filter/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/spam-filter/</guid>
      <description>Perform the SMS spam filtering analysis.
Step 1: Load dataStep 2: Exploring and preparing the data# read the sms data into the sms data framesms_raw &amp;lt;- read.csv(&amp;quot;C:/Users/Bhawna/Documents/blog/data/sms_spam.csv&amp;quot;, stringsAsFactors = FALSE)# examine the structure of the sms datastr(sms_raw)## &amp;#39;data.frame&amp;#39;: 5559 obs. of 2 variables:## $ type: chr &amp;quot;ham&amp;quot; &amp;quot;ham&amp;quot; &amp;quot;ham&amp;quot; &amp;quot;spam&amp;quot; ...## $ text: chr &amp;quot;Hope you are having a good week. Just checking in&amp;quot; &amp;quot;K.</description>
    </item>
    
    <item>
      <title>Support Vector Machine</title>
      <link>/post/support-vector-machine/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/support-vector-machine/</guid>
      <description>The main purpose of the Support Vector Machine(SVM) method is to identify a hyperplane which separates the samples. This technique can be used for both classification and numerical prediction. Usually used for only binary classification. Benefits include high scalability, but drawbacks, as for neural nets, include models which are very difficult to interpret.
Plot below help visualize overall way to visualize SVM technques.The hyperplane is a multidimensional surface, but it’s easiest to think of it as a straight line between two linearly separable groups of examples.</description>
    </item>
    
    <item>
      <title>Times Series</title>
      <link>/post/times-series/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/times-series/</guid>
      <description># Load datapopulation &amp;lt;- read.csv(&amp;quot;C:/Users/Bhawna/Documents/blog/data/world-population.csv&amp;quot;, sep=&amp;quot;,&amp;quot;, header=TRUE)# Default plotplot(population$Year, population$Population, type=&amp;quot;l&amp;quot;)# Adjust axisplot(population$Year, population$Population, type=&amp;quot;l&amp;quot;, ylim=c(0, 7000000000), xlab=&amp;quot;Year&amp;quot;, ylab=&amp;quot;Population&amp;quot;)Reference: world_population.csv was obtained from Flowdata.com</description>
    </item>
    
    <item>
      <title>How does the Bootstrap method work?</title>
      <link>/post/bootstrap/</link>
      <pubDate>Sun, 11 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/bootstrap/</guid>
      <description>Question of the day?Suppose we have a normal population with a mean of 3 and a standard deviation of 1, how would we illustrate the bootstrap method here?
set.seed(1)srs &amp;lt;- rnorm(25, mean=3)resamps &amp;lt;- replicate(1000, sample(srs, 25, TRUE), simplify = FALSE)x_bar_star &amp;lt;- sapply(resamps, mean, simplify =TRUE)Let’s create a histogram here and fit normal curve.
hist(x_bar_star, breaks = 40, prob = TRUE)curve(dnorm(x, 3, 0.2), add = TRUE) We can calculate the difference now.</description>
    </item>
    
  </channel>
</rss>